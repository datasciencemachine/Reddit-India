{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/abhishek-parashar/Reddit-flair-detection/blob/master/web_scrapping_and_pre_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cnCdGY4gMKoW"
   },
   "source": [
    "## scrapping the reddit data from reddit.\n",
    "### references- \n",
    "#### 1. https://www.youtube.com/watch?v=NRgfgtzIhBQ&t=103s\n",
    "#### 2. https://www.storybench.org/how-to-scrape-reddit-with-python/\n",
    "#### 3. https://towardsdatascience.com/scraping-reddit-data-1c0af3040768\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9u9lXAftL2he"
   },
   "outputs": [],
   "source": [
    "# getting the data from reddit using praw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "colab_type": "code",
    "id": "GMeOmo8XOkFB",
    "outputId": "c7f580c2-529b-4b13-bb8c-0d3dbcc12f08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/c0/b9714b4fb164368843b41482a3cac11938021871adf99bf5aaa3980b0182/praw-6.5.1-py3-none-any.whl (134kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 3.4MB/s \n",
      "\u001b[?25hCollecting update-checker>=0.16\n",
      "  Downloading https://files.pythonhosted.org/packages/17/c9/ab11855af164d03be0ff4fddd4c46a5bd44799a9ecc1770e01a669c21168/update_checker-0.16-py2.py3-none-any.whl\n",
      "Collecting websocket-client>=0.54.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 10.5MB/s \n",
      "\u001b[?25hCollecting prawcore<2.0,>=1.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/76/b5/ce6282dea45cba6f08a30e25d18e0f3d33277e2c9fcbda75644b8dc0089b/prawcore-1.0.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from update-checker>=0.16->praw) (2.21.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client>=0.54.0->praw) (1.12.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (2019.11.28)\n",
      "Installing collected packages: update-checker, websocket-client, prawcore, praw\n",
      "Successfully installed praw-6.5.1 prawcore-1.0.1 update-checker-0.16 websocket-client-0.57.0\n"
     ]
    }
   ],
   "source": [
    "!pip install praw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YwGmQ82Z2_-x"
   },
   "source": [
    "importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "iCZPzVtoO3Qm",
    "outputId": "3585870c-2a9b-408c-a185-8c9edfaa6441"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import datetime as dt\n",
    "#nltk.download('all')\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qIUsmojcNf88"
   },
   "source": [
    "for getting the reddit data from praw we need to enter the credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YVmswRpeNe-i"
   },
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='8-xCwnG8N3QJOw', client_secret='Hu5luIlZWsHED__mLEqAYhoYfVOxSQ', user_agent='r/inidia', username='Dororo_hyaki')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6uiskvjvZsqF"
   },
   "source": [
    "### getting the subreddits and the topics and flairs\n",
    "### labels_dict stores all the parts of reddit post that is flair, id, url, body, comments etc.\n",
    "### flair contails all the list of flairs of the subreddit India."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i9heWAt4Zrgl"
   },
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit('india')\n",
    "labels_dict = {\"flair\":[], \"title\":[], \"score\":[], \"id\":[], \"url\":[], \"comms_num\": [], \"created\": [], \"body\":[], \"author\":[], \"comments\":[]}\n",
    "flairs = [\"AskIndia\", \"Non-Political\", \"[R]eddiquette\", \"Scheduled\", \"Photography\", \"Science/Technology\", \"Politics\", \"Business/Finance\", \"Policy/Economy\", \"Sports\", \"Food\", \"AMA\",\"Coronavirus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7xUWd0nZask6"
   },
   "source": [
    "### appending the scrapped data into the dictionary\n",
    "### we iterate through all the flairs and append the url, body, score, flairs etc in the dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CwcfG87yaqzE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awRbYyB21XEN"
   },
   "source": [
    "### A Datetime library is used to append the time stamp of the reddit posts. \n",
    "### Created a function to use it further in the data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bOUsx9ABe8Fb"
   },
   "outputs": [],
   "source": [
    "def get_date(created):\n",
    "    return dt.datetime.fromtimestamp(created)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gxsm3X3gbTNC"
   },
   "source": [
    "### converting into dataframe and saving the csv file \n",
    "### pandas is used to create the data frame and then the time stamp is appended by using the function created above. \n",
    "### at last the data is saved in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tg-xy2rta8Dj"
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(labels_dict)\n",
    "time =data[\"created\"].apply(get_date)\n",
    "data =data.assign(timestamp = time)\n",
    "del data['created']\n",
    "data.to_csv('reddit-india-data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GI4_T2z4aCdn"
   },
   "source": [
    "observing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 652
    },
    "colab_type": "code",
    "id": "B5521MwBaLSy",
    "outputId": "5489693a-6392-4607-a4f7-b27d13978271"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flair</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "      <th>comments</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AskIndia</td>\n",
       "      <td>4 days ago we had pending orders of 100 millio...</td>\n",
       "      <td>95</td>\n",
       "      <td>fwjdqr</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/fwjdqr...</td>\n",
       "      <td>6</td>\n",
       "      <td>&gt; We are getting frantic calls from our pharma...</td>\n",
       "      <td>india_ko_vanakkam</td>\n",
       "      <td>Modi has Stockholm syndrome To be fair, the e...</td>\n",
       "      <td>2020-04-07 20:07:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AskIndia</td>\n",
       "      <td>Randians who were big time users of dating app...</td>\n",
       "      <td>18</td>\n",
       "      <td>fizkkk</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/fizkkk...</td>\n",
       "      <td>19</td>\n",
       "      <td>I'd my own stint with these apps(a couple of m...</td>\n",
       "      <td>__knockknockturnal__</td>\n",
       "      <td>Someone matched with me just to tell me that ...</td>\n",
       "      <td>2020-03-15 18:48:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AskIndia</td>\n",
       "      <td>What does r/India thinks about the Flat Earthers?</td>\n",
       "      <td>4</td>\n",
       "      <td>f25vx0</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/f25vx0...</td>\n",
       "      <td>31</td>\n",
       "      <td>I encountered a Foreigner in IG who says \" Rou...</td>\n",
       "      <td>Dev1003</td>\n",
       "      <td>I haven't found a Indian yet who believes ear...</td>\n",
       "      <td>2020-02-11 17:10:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AskIndia</td>\n",
       "      <td>People who left their 9 to 5 jobs to pursue a ...</td>\n",
       "      <td>45</td>\n",
       "      <td>dtvliq</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/dtvliq...</td>\n",
       "      <td>34</td>\n",
       "      <td>Couldn't add AskIndia flair from the mobile br...</td>\n",
       "      <td>c0mrade34</td>\n",
       "      <td>An Engineer, doing advertisement shoots since...</td>\n",
       "      <td>2019-11-09 20:57:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AskIndia</td>\n",
       "      <td>Somebody want to kill my full family what to do?</td>\n",
       "      <td>92</td>\n",
       "      <td>b7pvwt</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/b7pvwt...</td>\n",
       "      <td>24</td>\n",
       "      <td>It's now 24hrs, But local police station is no...</td>\n",
       "      <td>amitkumarthakur</td>\n",
       "      <td>Calm down.\\nGo to the SP office of your town,...</td>\n",
       "      <td>2019-04-01 01:00:35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      flair  ...            timestamp\n",
       "0  AskIndia  ...  2020-04-07 20:07:04\n",
       "1  AskIndia  ...  2020-03-15 18:48:06\n",
       "2  AskIndia  ...  2020-02-11 17:10:55\n",
       "3  AskIndia  ...  2019-11-09 20:57:35\n",
       "4  AskIndia  ...  2019-04-01 01:00:35\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('reddit-india-data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HDuL-vJIaf4d"
   },
   "source": [
    "After observing the data it's obvious data needs some pre processing for comprehending further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nlGM8ZBycX8R"
   },
   "source": [
    "## applying string\n",
    "### earlier i was applying str() in the function itself but was getting an error so made a function for it \n",
    "### the error can be found at log manager \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VE6y6BbC9L4Q"
   },
   "outputs": [],
   "source": [
    "def string(value):\n",
    "    return str(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uhLXrCpEfpke"
   },
   "outputs": [],
   "source": [
    "data['title'] = data['title'].apply(string)\n",
    "data['body'] = data['body'].apply(string)\n",
    "data['comments'] = data['comments'].apply(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "724blsWUdFmg"
   },
   "source": [
    "text cleaning that is replacing bad words, changing it to lower etc\n",
    "This needs another library Re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G9ILaLW9btlA"
   },
   "outputs": [],
   "source": [
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def text_cleaning(text):\n",
    "   \n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = text.lower()\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "    text = BAD_SYMBOLS_RE.sub('', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dMyouTyDbiwJ"
   },
   "outputs": [],
   "source": [
    "data['title'] = data['title'].apply(text_cleaning)\n",
    "data['body'] = data['body'].apply(text_cleaning)\n",
    "data['comments'] = data['comments'].apply(text_cleaning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZqVGNd9TdQcq"
   },
   "source": [
    "combining the clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ms3ZNBSzbi-5"
   },
   "outputs": [],
   "source": [
    "combined_features = data[\"title\"] + data[\"comments\"] + data[\"url\"] + data[\"body\"]\n",
    "data = data.assign(combined_features = combined_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6wmmzfl7d1k9"
   },
   "source": [
    "saving the new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Efl_BnEMd1D1"
   },
   "outputs": [],
   "source": [
    "data.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jmKlHOK3LQm4"
   },
   "source": [
    "looking at the final data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CdloKZ13eOU3",
    "outputId": "54c54859-c516-4c4e-a254-1a6170c7de58"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>flair</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "      <th>comments</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>combined_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>4 days ago pending orders 100 million hydroxyc...</td>\n",
       "      <td>95</td>\n",
       "      <td>fwjdqr</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/fwjdqr...</td>\n",
       "      <td>6</td>\n",
       "      <td>getting frantic calls pharma customers delayed...</td>\n",
       "      <td>india_ko_vanakkam</td>\n",
       "      <td>modi stockholm syndrome fair evidence chloroqu...</td>\n",
       "      <td>2020-04-07 20:07:04</td>\n",
       "      <td>4 days ago pending orders 100 million hydroxyc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>randians big time users dating apps like tinde...</td>\n",
       "      <td>18</td>\n",
       "      <td>fizkkk</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/fizkkk...</td>\n",
       "      <td>19</td>\n",
       "      <td>id stint apps couple months one point didnt fe...</td>\n",
       "      <td>__knockknockturnal__</td>\n",
       "      <td>someone matched tell im fat cat 1 general foll...</td>\n",
       "      <td>2020-03-15 18:48:06</td>\n",
       "      <td>randians big time users dating apps like tinde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>r india thinks flat earthers</td>\n",
       "      <td>4</td>\n",
       "      <td>f25vx0</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/f25vx0...</td>\n",
       "      <td>31</td>\n",
       "      <td>encountered foreigner ig says round earth hoax...</td>\n",
       "      <td>Dev1003</td>\n",
       "      <td>havent found indian yet believes earth flat de...</td>\n",
       "      <td>2020-02-11 17:10:55</td>\n",
       "      <td>r india thinks flat earthershavent found india...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>people left 9 5 jobs pursue career music art f...</td>\n",
       "      <td>45</td>\n",
       "      <td>dtvliq</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/dtvliq...</td>\n",
       "      <td>34</td>\n",
       "      <td>couldnt add askindia flair mobile browser</td>\n",
       "      <td>c0mrade34</td>\n",
       "      <td>engineer advertisement shoots since last 1year...</td>\n",
       "      <td>2019-11-09 20:57:35</td>\n",
       "      <td>people left 9 5 jobs pursue career music art f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>somebody want kill full family</td>\n",
       "      <td>92</td>\n",
       "      <td>b7pvwt</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/b7pvwt...</td>\n",
       "      <td>24</td>\n",
       "      <td>24hrs local police station register case dont ...</td>\n",
       "      <td>amitkumarthakur</td>\n",
       "      <td>calm downgo sp office town file grievance imme...</td>\n",
       "      <td>2019-04-01 01:00:35</td>\n",
       "      <td>somebody want kill full familycalm downgo sp o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>1213</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>coronavirus india hunger real problem</td>\n",
       "      <td>15</td>\n",
       "      <td>fwk2iv</td>\n",
       "      <td>https://www.rediff.com/news/interview/coronavi...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mulaayam_Yadav</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-04-07 20:59:02</td>\n",
       "      <td>coronavirus india hunger real problemnanhttps:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>1214</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>need investigate nso group behind pegasus soft...</td>\n",
       "      <td>52</td>\n",
       "      <td>fyawr9</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/fyawr9...</td>\n",
       "      <td>2</td>\n",
       "      <td>https previewreddit p3vb8eexzxr41pngwidth1024f...</td>\n",
       "      <td>InternetFreedomIn</td>\n",
       "      <td>nso group isreal would say nso make softwares ...</td>\n",
       "      <td>2020-04-10 15:25:37</td>\n",
       "      <td>need investigate nso group behind pegasus soft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>1215</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>coronavirus dont target doctors asking protect...</td>\n",
       "      <td>71</td>\n",
       "      <td>fwcvg1</td>\n",
       "      <td>https://www.thehindu.com/news/national/coronav...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>naveen_reloaded</td>\n",
       "      <td>thali nhi bajai kya aur diya nhi jalaya kya hu...</td>\n",
       "      <td>2020-04-07 11:08:38</td>\n",
       "      <td>coronavirus dont target doctors asking protect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>1216</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>india lifts export ban potential coronavirus d...</td>\n",
       "      <td>12</td>\n",
       "      <td>fwkrox</td>\n",
       "      <td>https://asia.nikkei.com/Spotlight/Coronavirus/...</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vinacham</td>\n",
       "      <td>sed bhakt noises deleted request let trump the...</td>\n",
       "      <td>2020-04-07 21:46:06</td>\n",
       "      <td>india lifts export ban potential coronavirus d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217</th>\n",
       "      <td>1217</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>33 #covid19 positive cases reported haryana to...</td>\n",
       "      <td>13</td>\n",
       "      <td>fwkiil</td>\n",
       "      <td>https://twitter.com/ANI/status/124751605800240...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VivaLatino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-04-07 21:29:03</td>\n",
       "      <td>33 #covid19 positive cases reported haryana to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1218 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  ...                                  combined_features\n",
       "0              0  ...  4 days ago pending orders 100 million hydroxyc...\n",
       "1              1  ...  randians big time users dating apps like tinde...\n",
       "2              2  ...  r india thinks flat earthershavent found india...\n",
       "3              3  ...  people left 9 5 jobs pursue career music art f...\n",
       "4              4  ...  somebody want kill full familycalm downgo sp o...\n",
       "...          ...  ...                                                ...\n",
       "1213        1213  ...  coronavirus india hunger real problemnanhttps:...\n",
       "1214        1214  ...  need investigate nso group behind pegasus soft...\n",
       "1215        1215  ...  coronavirus dont target doctors asking protect...\n",
       "1216        1216  ...  india lifts export ban potential coronavirus d...\n",
       "1217        1217  ...  33 #covid19 positive cases reported haryana to...\n",
       "\n",
       "[1218 rows x 12 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQrsqRXiLYsZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyM54SqA8TMwu7eUxBkLhEDX",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "web scrapping and pre processing",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
